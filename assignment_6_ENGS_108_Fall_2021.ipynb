{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_6_ENGS_108_Fall_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teshiebronk/teshieColab/blob/main/assignment_6_ENGS_108_Fall_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BiHNk7HcRiL"
      },
      "source": [
        "# **ENGS 108 Fall 2021 Assignment 6**\n",
        "\n",
        "*Due October 23 2021\n",
        "\n",
        "**Instructors:** George Cybenko\n",
        "\n",
        "**TAs:** Clement Nyanhongo, Jack Sadoff\n",
        "\n",
        "**Student** Teshie Bronk\n",
        "---\n",
        "\n",
        "## **Rules and Requirements**\n",
        "\n",
        "\n",
        "1.   You are only allowed to use Python packages that are explicity imported in \n",
        "the assignment notebook or are standard (bultin) python libraries like random, os, sys, etc, (Standard Bultin Python libraries will have a Python.org documentation). For this assignment you may use:\n",
        "  *   [numpy](https://numpy.org/doc/stable/)\n",
        "  *   [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
        "  *   [scikit-learn](https://scikit-learn.org/stable/)\n",
        "  *   [matplotlib](https://matplotlib.org/)\n",
        "  *   [tensorflow](https://www.tensorflow.org/)\n",
        "\n",
        "2.   All code must be fit into the designated code or text blocks in the assignment notebook. They are indentified by a **TODO** qualifier.\n",
        "\n",
        "3. For analytical questions that don't require code, type your answer cleanly in Markdown. For help, see the [Google Colab Markdown Guide](https://colab.research.google.com/notebooks/markdown_guide.ipynb).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KqjPtWlcFnS"
      },
      "source": [
        "''' Import Statements '''\n",
        "from collections import defaultdict\n",
        "import copy\n",
        "import itertools\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import random\n",
        "# Don't mess with this (gives reproducible results)\n",
        "random.seed(444)"
      ],
      "execution_count": 354,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0otRQ4_CcxP4"
      },
      "source": [
        "## **Problem 1: Reinforcement Learning**\n",
        "In this problem we will play a game of cops and robbers. The game is played on a fixed undirected, simple, and finite graph $G$. There are two players, a cop and a robber. It is the goal of the cop to catch the robber in as few moves as possible. \n",
        "\n",
        "The graph $G$ has the following properties:\n",
        "  - A total of $m$ nodes.\n",
        "  - It contains a single $n$ node cycle, where $n\\leq m$, and random additional edges to make the graph connected.\n",
        "\n",
        "The game starts, with the cop taking their choice of vertex in $G$ and then the robber selects a random vertex in $G$ that is not occupied by the cop. At every point in the game both players know the positions of each other, and in this version of the problem we will say that the robber is drunk (i.e. they will randomly choose there next that instead of employing a policy).\n",
        "\n",
        "The availabe actions of the cop and associated reward function is:\n",
        "  - Move to a node not connected to their present node (and the cop stays in the current position): -5.\n",
        "  - Move to an adjacent node (including staying at current node): -1.\n",
        "  - Move to the node occupied by robber: +100.\n",
        ">\n",
        "> **Part 1** Building a Graph Class.\n",
        ">> **(a)** Using the provided skeleton build a general graph class for this problem for $n$ nodes. You are expected to implement *add_edge*, *make_random_graph*, *check_connected*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaHjFProBqKc"
      },
      "source": [
        "class Graph:\n",
        "  \"\"\" Our graph class.\n",
        "  Args:\n",
        "    n: Number of nodes in our cycle.\n",
        "    m: Total number of nodes in graph, where n >= m.\n",
        "  \"\"\"\n",
        "  def __init__(self, n, m):\n",
        "    self.n = n\n",
        "    self.m = m\n",
        "    # A default dict is just a dict that won't raise a KeyError, it instead fillsds\n",
        "    # the unknown key with a default, in our case an empty list.\n",
        "    self.G = defaultdict(list) #dictionary just links a key to some value\n",
        "    #difference between this and a regular dictionary \n",
        "    #key is node and this is linked to list of everything its attached\n",
        "\n",
        "    \n",
        "  def add_edge(self, u, v):\n",
        "    \"\"\"\n",
        "    Make a function that will add an edge to the graph.\n",
        "    \"\"\"\n",
        "    self.G[u].append(v)\n",
        "    self.G[v].append(u)\n",
        "\n",
        "\n",
        "  def make_random_graph(self):\n",
        "    \"\"\" First make a cycle of given length and then add random additional edges\n",
        "    in such a way that the final graph will be connected.\n",
        "    \"\"\"\n",
        "    #TODO: Make n length cycle first\n",
        "    print(\"here\")\n",
        "    for i in range((self.n)-1):\n",
        "      if i == self.n -1:\n",
        "        print(\"m\")\n",
        "        continue #break out of loop\n",
        "      self.add_edge(i,i+1)\n",
        "    self.add_edge(0,self.n-1) #ie. make the final to first connection\n",
        "\n",
        "\n",
        "    #TODO: Add additional nodes until random graph is connected\n",
        "    total_ext = self.m\n",
        "    counter = self.n\n",
        "    while counter < total_ext:\n",
        "      _graph = copy.copy(self)\n",
        "      #TODO: Use the graph copy to add random nodes/edges\n",
        "      node_to_add_on = random.randint(0, self.n -1)\n",
        "      if len(self.G[node_to_add_on]) != 2:\n",
        "        temp_node = self.G[node_to_add_on][-1]\n",
        "        self.add_edge(temp_node,counter)\n",
        "      else:\n",
        "        self.add_edge(node_to_add_on,counter)\n",
        "      counter = counter + 1\n",
        "\n",
        "    if self.check_connected(_graph.G):\n",
        "      # If it is set the current graph's adjacency matrix equal to the copy's.\n",
        "      print('true')\n",
        "      self.graph = _graph.G\n",
        "      # Return (i.e. break the loop)\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "  def check_connected(self, G):\n",
        "    \"\"\" Perform a Depth First Search.\n",
        "    \"\"\"\n",
        "    start_node = random.choice(list(G.keys()))\n",
        "    return dfs(G, start_node, self.m)\n",
        "\n",
        "def dfs(G,u,m):\n",
        "  \"\"\" Implement your own depth first search. Many online resources for this.\n",
        "  Args:\n",
        "    G: Is the graph (adjency matrix) we want to test.\n",
        "    visited: Is a dictionary that keeps track of the nodes you've visited.\n",
        "    u: a starting node.\n",
        "    parent: The parent node for DFS so that you can cancel recursion if you complete the cycle.\n",
        "  Returns:\n",
        "    True: If all nodes have been visited at least once.\n",
        "    False: Otherwise.\n",
        "  \"\"\"\n",
        "\n",
        "  #algorithm that follows a path and marks nodes that it's visited and ensures that its visited every node on the graph \n",
        "  #TODO: Implement. Hint: Make sure to keep track of the parent node for a DFS search,\n",
        "  # as a cycle leads to an infinite recursion, so you have to make sure you break the cycle.\n",
        "  stack = [] #creates list stack\n",
        "  stack.append(u)\n",
        "  visited = set()\n",
        "  while stack:\n",
        "    curr = stack.pop()#pulling the last one added on the stack and deleting it    \n",
        "    visited.add(curr) \n",
        "    if len(visited) == m:\n",
        "      return True\n",
        "    all_neighbors = G[curr]  #neighbros of G[i]\n",
        "    for neighbor in all_neighbors:\n",
        "      if neighbor not in visited:\n",
        "        stack.append(neighbor)\n",
        "  \n",
        "  return False \n",
        "\n"
      ],
      "execution_count": 355,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc44XK7_JEYO"
      },
      "source": [
        ">> **(b)** Test out your graph class with a cops and robbers graph of $n=5$ and $m=10$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKxMuq_7BkGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d93209-182b-4112-ee17-c091b0de6ef4"
      },
      "source": [
        "#TODO: Your code goes here.\n",
        "g = Graph(5,10)\n",
        "bl = g.make_random_graph()\n",
        "print(g.G)\n",
        "#Great this appears to work. This prints each node with its associated connected nodes. "
      ],
      "execution_count": 357,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n",
            "true\n",
            "defaultdict(<class 'list'>, {0: [1, 4], 1: [0, 2], 2: [1, 3, 5], 3: [2, 4, 7], 4: [3, 0, 6], 5: [2, 9], 6: [4, 8], 7: [3], 8: [6], 9: [5]})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTuchWzJJaxk"
      },
      "source": [
        "> **Part 2** Understanding the state space, i.e. the Game.\n",
        ">> **(a)** Given the graph class you've created in Part 1. Develop a Cops and Robbers game class. Use the skeleton below to implement the following functions first: *get_successors*, *terminal_test*, *result*. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qBoFrNDBT9y"
      },
      "source": [
        "class CopsAndRobbers:\n",
        "  def __init__(self, graph, start_state, rewards_table=None):\n",
        "    \"\"\" This is the cops and robbers game class.\n",
        "    Args:\n",
        "      start_state: The starting state of the cop position and robber position in\n",
        "        the graph. Should be a tuple of form (cop_pos, rob_pos).\n",
        "      G: The graph adjacency matrix\n",
        "      state: current state in the game.\n",
        "      reward_table: The (state, actions) reward dictionary that you will eventually implement.\n",
        "    \"\"\"\n",
        "    self.graph = graph\n",
        "    self.G = graph.G\n",
        "    self.state = start_state\n",
        "    self.rewards_table = rewards_table\n",
        "  \n",
        "  \n",
        "  def terminal_test(self, state):\n",
        "    #TODO: Implement.\n",
        "    if self.state[0] == self.state[1]: \n",
        "      return True\n",
        "    return False \n",
        "\n",
        "  def get_successors(self, state):\n",
        "    \"\"\" Return a list of successor states that can be reached from the current state.\n",
        "    Hint: Remember only the cop can choose their action.\n",
        "    \"\"\"\n",
        "    #TODO: Implement.\n",
        "    return (self.G[state[0]])\n",
        "    #self.G[state[1]]\n",
        "\n",
        "  def result_rob(self, state):\n",
        "    \"\"\" This function should return the state after the cop has made their move,\n",
        "    and the drunk robber has moved accordingly.\n",
        "    Args:\n",
        "      state: Current state of (cop, rob).\n",
        "      next_state: The state after the cop has moved (next_cop, rob). Calculated from get_successors.\n",
        "    \"\"\"\n",
        "    # TODO: Implement.\n",
        "    next_state_rob = random.choice(self.G[state[1]]) # and this is decided with the reward table\n",
        "    state_cop = self.state[0]\n",
        "    self.state = (state_cop,next_state_rob)\n",
        "    #return (next_state_cop, next_state_rob)\n",
        "\n",
        "  def result_cop(self, state, next_state):\n",
        "    next_state_cop = next_state# and this is decided with the reward table\n",
        "    state_rob = self.state[1]\n",
        "    self.state = (next_state_cop,state_rob)\n",
        "    #return (next_state_cop, next_state_rob)\n",
        "\n",
        "#The availabe actions of the cop and associated reward function is:\n",
        "#Move to a node not connected to their present node (and the cop stays in the current position): -5.\n",
        "#Move to an adjacent node (including staying at current node): -1.\n",
        "#Move to the node occupied by robber: +100.\n",
        "  def utility(self, state, action):\n",
        "    return self.rewards_table[(state, action)]"
      ],
      "execution_count": 358,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40wpcZOOVsBU",
        "outputId": "95bdd36e-0de5-4fc1-cc0a-8ca1982160c9"
      },
      "source": [
        "c = CopsAndRobbers(g,(0,4),None)\n",
        "print(c.get_successors((0,2)))"
      ],
      "execution_count": 359,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-YjCqNsXOKG"
      },
      "source": [
        ">> **(b)** In reinforcement learning we are often interested in calculating a rewards table that has possible states as its rows and possible actions as its columns and filled in with the associated reward given the Q(state, action) pair. Calculate the rewards table for any given graph. *Hint: This should be an $m^2$ x $m$ matrix or a dictionary with $m^3$ keys such that the keys are (state, action) tuples.*  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F61FjlM__wEg"
      },
      "source": [
        "def calculateRewardsTable(graph):\n",
        "  \"\"\" Make a rewards table dictionary of the from table[(state, action)] = reward.\n",
        "  \"\"\"\n",
        "  #TODO: implement\n",
        "  #states\n",
        "  table = defaultdict(list)\n",
        "\n",
        "  for cop in range(graph.m):\n",
        "    for rob in range(graph.m):\n",
        "      for action in range(graph.m):\n",
        "        if (action in graph.G[cop]) & (action == rob):\n",
        "          table[(cop,rob),action] = 100\n",
        "        elif (action in graph.G[cop]) & (action != rob):\n",
        "          table[(cop,rob),action] = -1\n",
        "        else: \n",
        "          table[(cop,rob),action] = -5\n",
        "  return table"
      ],
      "execution_count": 360,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhXhfdSkjY94",
        "outputId": "e033d525-5c8e-4da8-cb18-2489f2876bec"
      },
      "source": [
        "aa = calculateRewardsTable(g)\n",
        "len(aa)"
      ],
      "execution_count": 361,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 361
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU0Hq2C8bocS"
      },
      "source": [
        ">> **(c)** Now that we have our reward table, try to solve the problem in a brute-force manner (without reinforcement learning). I.e. try to reach the terminal state, or find get a reward of 100. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GysY6hjZ_Iu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "138287cb-8284-424c-bf09-2bd842c06da2"
      },
      "source": [
        "epochs = 0\n",
        "rewards = []\n",
        "penalties = 0\n",
        "#TODO: Instantiate your graph\n",
        "g = Graph(5,10)\n",
        "g.make_random_graph()\n",
        "\n",
        "#TODO: Build your rewards table\n",
        "Rewards = calculateRewardsTable(g)\n",
        "\n",
        "#TODO: Instantiate your game.\n",
        "#arbitrary start state\n",
        "start_state = (2,7)\n",
        "game = CopsAndRobbers(g,start_state,Rewards)\n",
        "\n",
        "# Simulation loop\n",
        "playing = True\n",
        "penalty = 0\n",
        "while playing:\n",
        "  #TODO: Get the next state, check for terminal state, and get reward\n",
        "  next_state_cop = random.randint(0, g.m - 1)\n",
        "  if next_state_cop in game.G[game.state[0]]: #is our next_state for the cop valid from our current one\n",
        "    penalty = penalty + game.utility(game.state,next_state_cop) #add penalty from the rewards table\n",
        "    game.result_cop(game.state,next_state_cop)\n",
        "    playing = not game.terminal_test(game.state)\n",
        "    if playing == False:\n",
        "      continue\n",
        "      print(\"playing\")\n",
        "    game.result_rob(game.state)\n",
        "    playing = not game.terminal_test(game.state) #this checks if robber moved \n",
        "    if playing == False:\n",
        "      penalty == penalty + 100 # adds points because the robber landed on the cop \n",
        "      print(\"playing\")\n",
        "      continue\n",
        "    \n",
        "  else:\n",
        "    penalty = penalty + game.utility(game.state,next_state_cop)\n",
        "    game.result_rob(game.state)\n",
        "    playing = not game.terminal_test(game.state)\n",
        "    if playing == False:\n",
        "      penalty = penalty+100\n",
        "\n",
        "  # Make sure to break if you reach the terminal state."
      ],
      "execution_count": 362,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n",
            "true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhU4RvNR9O0R",
        "outputId": "17850818-c362-41d3-eeb4-d658a11540fa"
      },
      "source": [
        "print(game.state)\n",
        "print(penalty)\n"
      ],
      "execution_count": 363,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 2)\n",
            "85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRs6uy5OD90W"
      },
      "source": [
        "As you can see the game state reached the terminal state where the cop was on the robber and here is the associated penalty of that game."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec8eFd77jdiV"
      },
      "source": [
        "> **Part 3** Q-learning. \n",
        ">\n",
        "> Up to this point we have build a graph class, built a game class, ran a brute-force simulation of an agent traversing the space randomly, and now we will dive into Q-learning in the hopes of maximizing the rewards and efficiency of capturing the drunk robber. \n",
        ">\n",
        ">> **(a)** Using the skeleton from the brute force method, implement a training loop to learn a Q-table for a given graph and game. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhUNoESeLOgN",
        "outputId": "fe430ef3-acef-4f7b-d3b9-2fe7fceea9d3"
      },
      "source": [
        "#new rewards table\n",
        "n = 5\n",
        "m = 10\n",
        "\n",
        "q_graph = Graph(n, m)\n",
        "q_graph.make_random_graph()\n",
        "\n",
        "#define the shape of the environment (i.e., its states)\n",
        "environment_rows = m\n",
        "environment_columns = m\n",
        "\n",
        "#Create a 3D numpy array to hold the current Q-values for each state and action pair: Q(s, a) \n",
        "#The array contains 11 rows and 11 columns (to match the shape of the environment), as well as a third \"action\" dimension.\n",
        "#The \"action\" dimension consists of 4 layers that will allow us to keep track of the Q-values for each possible action in\n",
        "#each state (see next cell for a description of possible actions). \n",
        "#The value of each (state, action) pair is initialized to 0.\n",
        "q_values = np.zeros((environment_rows, environment_columns, m))\n",
        "#Create a 2D numpy array to hold the rewards for each state. \n",
        "#The array contains 11 rows and 11 columns (to match the shape of the environment), and each value is initialized to -1.\n",
        "rewards = np.full((m,m,m), -5.) #defaults to -5\n",
        "\n",
        "for cop in range(m):\n",
        "  for rob in range(m):\n",
        "    for action in range(m):\n",
        "      if (action in q_graph.G[cop]) & (action == rob):\n",
        "        rewards[cop,rob,action] = 100\n",
        "      elif (action in q_graph.G[cop]) & (action != rob):\n",
        "        rewards[cop,rob,action] = -1\n"
      ],
      "execution_count": 391,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n",
            "true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDFIKxth85FA"
      },
      "source": [
        "#define a function that determines if the specified location is a terminal state\n",
        "def is_terminal_state(current_row_index, current_column_index):\n",
        "  #if the reward for this location is -1, then it is not a terminal state (i.e., it is a 'white square')\n",
        "  if current_row_index == current_column_index:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": 392,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YJfejwOOXfD"
      },
      "source": [
        "#define a function that will choose a random, non-terminal starting location\n",
        "def get_starting_location():\n",
        "  #get a random row and column index\n",
        "  current_row_index = np.random.randint(environment_rows)\n",
        "  current_column_index = np.random.randint(environment_columns)\n",
        "  #continue choosing random row and column indexes until a non-terminal state is identified\n",
        "  #(i.e., until the chosen state is a 'white square').\n",
        "  while is_terminal_state(current_row_index, current_column_index):\n",
        "    current_row_index = np.random.randint(environment_rows)\n",
        "    current_column_index = np.random.randint(environment_columns)\n",
        "  return current_row_index, current_column_index"
      ],
      "execution_count": 393,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLOUeHnUOVuG"
      },
      "source": [
        "#define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
        "def get_next_action(current_row_index, current_column_index, epsilon):\n",
        "  #if a randomly chosen value between 0 and 1 is less than epsilon, \n",
        "  #then choose the most promising value from the Q-table for this state.\n",
        "  if np.random.random() < epsilon:\n",
        "    return np.argmax(q_values[current_row_index, current_column_index])\n",
        "  else: #choose a random action\n",
        "    return np.random.randint(m)"
      ],
      "execution_count": 394,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3kn63HNMkei"
      },
      "source": [
        "#define a function that will get the next location based on the chosen action\n",
        "def get_next_location(current_row_index, current_column_index, action_index):\n",
        "  new_row_index = current_row_index\n",
        "  new_column_index = current_column_index \n",
        "\n",
        "  if action_index in q_graph.G[current_row_index]:\n",
        "    new_row_index = action_index\n",
        "  if new_row_index != new_column_index: # only move if not terminal state\n",
        "    new_column_index = random.choice(q_graph.G[current_column_index]) \n",
        "\n",
        "  return new_row_index, new_column_index"
      ],
      "execution_count": 395,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsCb-lKnMvB2"
      },
      "source": [
        "#Define a function that will get the shortest path between any location within the warehouse that \n",
        "#the robot is allowed to travel and the item packaging location.\n",
        "def get_shortest_path(start_row_index, start_column_index):\n",
        "  #return immediately if this is an invalid starting location\n",
        "  if is_terminal_state(start_row_index, start_column_index):\n",
        "    return []\n",
        "  else: #if this is a 'legal' starting location\n",
        "    current_row_index, current_column_index = start_row_index, start_column_index\n",
        "    shortest_path = []\n",
        "    shortest_path.append([current_row_index, current_column_index])\n",
        "    #continue moving along the path until we reach the goal (i.e., the item packaging location)\n",
        "    while not is_terminal_state(current_row_index, current_column_index):\n",
        "      #get the best action to take\n",
        "      action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
        "      #move to the next location on the path, and add the new location to the list\n",
        "      current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
        "      shortest_path.append([current_row_index, current_column_index])\n",
        "    return shortest_path"
      ],
      "execution_count": 396,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK6LTfGNCzbO"
      },
      "source": [
        "#define training parameters\n",
        "epsilon = 0.9 #the percentage of time when we should take the best action (instead of a random action)\n",
        "discount_factor = 0.9 #discount factor for future rewards\n",
        "learning_rate = 0.9 #the rate at which the AI agent should learn\n",
        "\n",
        "#run through 100 training episodes and track the following factors\n",
        "reward_tot = 0\n",
        "reward_tracker =0\n",
        "steps = 0\n",
        "steps_tot = 0\n",
        "time_ave = 0\n",
        "t1 = 0\n",
        "for episode in range(100):\n",
        "  #get the starting location for this episode\n",
        "  row_index, column_index = get_starting_location() #gives a cop (row) and robber (col) loc\n",
        "  reward_tot = reward_tracker + reward_tot\n",
        "  steps_tot = steps_tot + steps\n",
        "  reward_tracker = 0 \n",
        "  steps = 0\n",
        "  time_ave = t1 + time_ave\n",
        "\n",
        "\n",
        "  #continue taking actions (i.e., moving) until we reach a terminal state\n",
        "  #(i.e., until we reach the item packaging area or crash into an item storage location)\n",
        "  t0= time.process_time()\n",
        "  while not is_terminal_state(row_index, column_index):\n",
        "    #choose which action to take (i.e., where to move next)\n",
        "    action_index = get_next_action(row_index, column_index, epsilon)\n",
        "\n",
        "    #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
        "    old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
        "    row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
        "    \n",
        "    #receive the reward for moving to the new state, and calculate the temporal difference\n",
        "    reward = rewards[old_row_index, old_column_index,action_index] #assign associated reward from the most recent action\n",
        "    reward_tracker = reward_tracker + reward\n",
        "    old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
        "    temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
        "\n",
        "    #update the Q-value for the previous state and action pair\n",
        "    new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
        "    q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
        "\n",
        "    steps = steps+1\n",
        "  t1 = time.process_time() - t0\n",
        "\n",
        "#average reward across 100 epochs\n",
        "reward_tot = reward_tot/100\n",
        "steps_tot = steps_tot/100\n",
        "time_ave = time_ave/100"
      ],
      "execution_count": 397,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FVu7FlNk3IA"
      },
      "source": [
        ">> **(b)** Evalute your new Q-learning agent over a 100 epochs, by choosing your actions based on the argmax of the Q-table caluculated in (a) and report the average number of penalities, average time, and average number of steps it took to find the robber with your new Q-learning strategy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqnDZQ3C-1qE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce3a477-98c4-4ad4-eab6-ead0f5218f58"
      },
      "source": [
        "print('Average Reward Per Epoch: ' +str(reward_tot))\n",
        "print('Average Steps Per Epoch: ' + str(steps_tot))\n",
        "print('Average Time Per Epoch: ' + str(time_ave)) "
      ],
      "execution_count": 398,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Reward Per Epoch: 48.52\n",
            "Average Steps Per Epoch: 6.39\n",
            "Average Time Per Epoch: 9.786122000093655e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnLCJHqUFT_l"
      },
      "source": [
        "Average Reward Per Epoch: 48.52\n",
        "\n",
        "Average Steps Per Epoch: 6.39\n",
        "\n",
        "Average Time Per Epoch: 9.786122000093655e-05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrpB9jKolvH_"
      },
      "source": [
        ">> **(c)** Compare your results with the brute-force method used in Part 2 and comment on the improvement. For instance, try varying graph configurations and look for any signs of improvement in certian instances. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ2HbYdKbmYV",
        "outputId": "eb427eda-c1bf-4aef-c739-257e9168d46c"
      },
      "source": [
        "#Create a test graph to apply to both the brute force and other method\n",
        "n = 5\n",
        "m = 10\n",
        "g = Graph(n,m)\n",
        "g.make_random_graph()\n",
        "#Run on Brute Force Method\n",
        "Rewards = calculateRewardsTable(g)\n",
        "\n",
        "#TODO: Instantiate your game.\n",
        "#arbitrary start state\n",
        "current_row_index = np.random.randint(m)\n",
        "current_column_index = np.random.randint(m)\n",
        "#continue choosing random row and column indexes until a non-terminal state is identified\n",
        "#(i.e., until the chosen state is a 'white square').\n",
        "while current_row_index == current_column_index:\n",
        "  current_row_index = np.random.randint(m)\n",
        "  current_column_index = np.random.randint(m)\n",
        "\n",
        "start_state = (current_row_index,current_column_index)\n",
        "game = CopsAndRobbers(g,start_state,Rewards)"
      ],
      "execution_count": 403,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n",
            "true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6b7G2mB_FIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b045ca-5e3c-4cfc-f884-f0ac307c4fe5"
      },
      "source": [
        "# Simulation loop\n",
        "tot_pen = 0\n",
        "tot_steps = 0\n",
        "#run for 100 iterations with the brute force method \n",
        "time_ave = 0\n",
        "for iter in range(100):\n",
        "  penalty = 0\n",
        "  tot_steps = steps + tot_steps\n",
        "  steps = 0\n",
        "  playing = True\n",
        "  t0 = time.process_time()\n",
        "  while playing:\n",
        "    #TODO: Get the next state, check for terminal state, and get reward\n",
        "    next_state_cop = random.randint(0, g.m - 1)\n",
        "    if next_state_cop in game.G[game.state[0]]: #is our next_state for the cop valid from our current one\n",
        "      penalty = penalty + game.utility(game.state,next_state_cop) #add penalty from the rewards table\n",
        "      game.result_cop(game.state,next_state_cop)\n",
        "      playing = not game.terminal_test(game.state)\n",
        "      if playing == False:\n",
        "        continue\n",
        "      game.result_rob(game.state)\n",
        "      playing = not game.terminal_test(game.state) #this checks if robber moved \n",
        "      if playing == False:\n",
        "        penalty == penalty + 100 # adds points because the robber landed on the cop \n",
        "        continue\n",
        "      \n",
        "    else:\n",
        "      penalty = penalty + game.utility(game.state,next_state_cop)\n",
        "      game.result_rob(game.state)\n",
        "      playing = not game.terminal_test(game.state)\n",
        "      if playing == False:\n",
        "        penalty = penalty+100\n",
        "    steps = steps+1\n",
        "  t1 = time.process_time() - t0\n",
        "  time_ave = t1 + time_ave \n",
        "  tot_pen = penalty + tot_pen\n",
        "    # Make sure to break if you reach the terminal state.\n",
        "tot_pen = tot_pen / 100\n",
        "tot_steps = tot_steps / 100 \n",
        "tot_time = t1 / 100\n",
        "time_ave = time_ave / 100\n",
        "\n",
        "print('Average Penalty Per Epoch: ' +str(tot_pen))\n",
        "print('Average Steps Per Epoch: ' + str(tot_steps))\n",
        "print('Average Time Per Epoch: ' + str(tot_time))"
      ],
      "execution_count": 404,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Penalty Per Epoch: 40.75\n",
            "Average Steps Per Epoch: 8.88\n",
            "Average Time Per Epoch: 8.071999999970103e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKRZZPa8fvWl"
      },
      "source": [
        "Q-Learn:\n",
        "\n",
        "Average Reward Per Epoch: 48.52\n",
        "\n",
        "Average Steps Per Epoch: 6.39\n",
        "\n",
        "Average Time Per Epoch: 9.786122000093655e-05\n",
        "\n",
        "\n",
        "Brute Force:\n",
        "\n",
        "Average Penalty Per Epoch: 40.75\n",
        "\n",
        "Average Steps Per Epoch: 8.88\n",
        "\n",
        "Average Time Per Epoch: 8.071999999970103e-08\n",
        "\n",
        "Using the brute force method with n = 5 and m = 10 we find that the reward is slightly lower than for the Q-learn when only iterating over 100 epochs. Around 8 steps are required per epoch which is more than that required for Q-learning. Finally, it should be noted that the time to run this approach is  faster, likely due to there being fewer function calls. Thus, for this few of epochs on a pretty uncomplex graph, the Q-learn doesn't significantly outperform the brute force approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUi6i8K7qVd1"
      },
      "source": [
        "Now let's test the brute force method against the Q-learning method with a larger graph and more epochs of training. Theoretically this is where Q-learning should really start to outperform brute. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3BsldHlfsdd",
        "outputId": "8676cae8-980a-4053-82fe-1b1c8a649b82"
      },
      "source": [
        "#Create a test graph to apply to both the brute force and other method\n",
        "\n",
        "#Try for 25 total nodes and 10 ext. \n",
        "n = 10\n",
        "m = 25\n",
        "g = Graph(n,m)\n",
        "g.make_random_graph()\n",
        "#Run on Brute Force Method\n",
        "Rewards = calculateRewardsTable(g)\n",
        "\n",
        "#TODO: Instantiate your game.\n",
        "#arbitrary start state\n",
        "current_row_index = np.random.randint(m)\n",
        "current_column_index = np.random.randint(m)\n",
        "#continue choosing random row and column indexes until a non-terminal state is identified\n",
        "#(i.e., until the chosen state is a 'white square').\n",
        "while current_row_index == current_column_index:\n",
        "  current_row_index = np.random.randint(m)\n",
        "  current_column_index = np.random.randint(m)\n",
        "\n",
        "start_state = (current_row_index,current_column_index)\n",
        "game = CopsAndRobbers(g,start_state,Rewards)"
      ],
      "execution_count": 405,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n",
            "true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkvgF3UOftzU",
        "outputId": "9528adae-0014-4e07-b572-00caf1eab05e"
      },
      "source": [
        "# Simulation loop\n",
        "tot_pen = 0\n",
        "tot_steps = 0\n",
        "#run for 100 iterations with the brute force method \n",
        "time_ave = 0\n",
        "for iter in range(100):\n",
        "  penalty = 0\n",
        "  tot_steps = steps + tot_steps\n",
        "  steps = 0\n",
        "  playing = True\n",
        "  t0 = time.process_time()\n",
        "  while playing:\n",
        "    #TODO: Get the next state, check for terminal state, and get reward\n",
        "    next_state_cop = random.randint(0, g.m - 1)\n",
        "    if next_state_cop in game.G[game.state[0]]: #is our next_state for the cop valid from our current one\n",
        "      penalty = penalty + game.utility(game.state,next_state_cop) #add penalty from the rewards table\n",
        "      game.result_cop(game.state,next_state_cop)\n",
        "      playing = not game.terminal_test(game.state)\n",
        "      if playing == False:\n",
        "        continue\n",
        "        print(\"playing\")\n",
        "      game.result_rob(game.state)\n",
        "      playing = not game.terminal_test(game.state) #this checks if robber moved \n",
        "      if playing == False:\n",
        "        penalty == penalty + 100 # adds points because the robber landed on the cop \n",
        "        continue\n",
        "      \n",
        "    else:\n",
        "      penalty = penalty + game.utility(game.state,next_state_cop)\n",
        "      game.result_rob(game.state)\n",
        "      playing = not game.terminal_test(game.state)\n",
        "      if playing == False:\n",
        "        penalty = penalty+100\n",
        "    steps = steps+1\n",
        "  t1 = time.process_time() - t0\n",
        "  time_ave = t1 + time_ave \n",
        "  tot_pen = penalty + tot_pen\n",
        "    # Make sure to break if you reach the terminal state.\n",
        "tot_pen = tot_pen / 100\n",
        "tot_steps = tot_steps / 100 \n",
        "tot_time = t1 / 100\n",
        "time_ave = time_ave / 100\n",
        "\n",
        "print('Average Penalty Per Epoch: ' +str(tot_pen))\n",
        "print('Average Steps Per Epoch: ' + str(tot_steps))\n",
        "print('Average Time Per Epoch: ' + str(tot_time))"
      ],
      "execution_count": 408,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Penalty Per Epoch: -5.02\n",
            "Average Steps Per Epoch: 21.85\n",
            "Average Time Per Epoch: 9.784999988937671e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiU_NFEWsZVa"
      },
      "source": [
        "Interestingly, when we increase the total nodes to 25, our penalty drops significantly (goes to around 0) and the total steps increase as well (around 20). This makes sense as we likely have many erroneous guesses during the brute force method. Time is not that different. Let's compare this to this larger graph for the q-learning method. See below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2AknH0Nyh2A",
        "outputId": "c0d381e4-a6d8-4bca-91c7-fa932e8d5d79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#new rewards table\n",
        "n = 10\n",
        "m = 25\n",
        "\n",
        "q_graph = Graph(n, m)\n",
        "q_graph.make_random_graph()\n",
        "\n",
        "#define the shape of the environment (i.e., its states)\n",
        "environment_rows = m\n",
        "environment_columns = m\n",
        "\n",
        "#Create a 3D numpy array to hold the current Q-values for each state and action pair: Q(s, a) \n",
        "#The array contains 11 rows and 11 columns (to match the shape of the environment), as well as a third \"action\" dimension.\n",
        "#The \"action\" dimension consists of 4 layers that will allow us to keep track of the Q-values for each possible action in\n",
        "#each state (see next cell for a description of possible actions). \n",
        "#The value of each (state, action) pair is initialized to 0.\n",
        "q_values = np.zeros((environment_rows, environment_columns, m))\n",
        "#Create a 2D numpy array to hold the rewards for each state. \n",
        "#The array contains 11 rows and 11 columns (to match the shape of the environment), and each value is initialized to -1.\n",
        "rewards = np.full((m,m, m), -5.)\n",
        "\n",
        "for cop in range(m):\n",
        "  for rob in range(m):\n",
        "    for action in range(m):\n",
        "      if (action in q_graph.G[cop]) & (action == rob):\n",
        "        rewards[cop,rob,action] = 100\n",
        "      elif (action in q_graph.G[cop]) & (action != rob):\n",
        "        rewards[cop,rob,action] = -1\n",
        "      #otherwise -5"
      ],
      "execution_count": 410,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n",
            "true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czuTZTQNs6ew",
        "outputId": "17bf303b-ee62-4bfb-d935-9f4998f62a4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "epsilon = 0.9 #the percentage of time when we should take the best action (instead of a random action)\n",
        "discount_factor = 0.9 #discount factor for future rewards\n",
        "learning_rate = 0.9 #the rate at which the AI agent should learn\n",
        "\n",
        "#run through 5000 training episodes\n",
        "reward_tot = 0\n",
        "reward_tracker =0\n",
        "steps = 0\n",
        "steps_tot = 0\n",
        "time_ave = 0\n",
        "t1 = 0\n",
        "for episode in range(5000):\n",
        "  #get the starting location for this episode\n",
        "  row_index, column_index = get_starting_location()\n",
        "  reward_tot = reward_tracker + reward_tot\n",
        "  steps_tot = steps_tot + steps\n",
        "  reward_tracker = 0 \n",
        "  steps = 0\n",
        "  time_ave = t1 + time_ave\n",
        "\n",
        "\n",
        "  #continue taking actions (i.e., moving) until we reach a terminal state\n",
        "  #(i.e., until we reach the item packaging area or crash into an item storage location)\n",
        "  t0= time.process_time()\n",
        "  while not is_terminal_state(row_index, column_index):\n",
        "    #choose which action to take (i.e., where to move next)\n",
        "    action_index = get_next_action(row_index, column_index, epsilon)\n",
        "\n",
        "    #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
        "    old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
        "    row_index, column_index = get_next_location(old_row_index, old_column_index, action_index)\n",
        "    \n",
        "    #receive the reward for moving to the new state, and calculate the temporal difference\n",
        "    reward = rewards[old_row_index, old_column_index,action_index]\n",
        "    reward_tracker = reward_tracker + reward\n",
        "    old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
        "    temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
        "\n",
        "    #update the Q-value for the previous state and action pair\n",
        "    new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
        "    q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
        "\n",
        "    steps = steps+1\n",
        "  t1 = time.process_time() - t0\n",
        "\n",
        "#average reward across 5000 epochs\n",
        "reward_tot = reward_tot/5000\n",
        "steps_tot = steps_tot/5000\n",
        "time_ave = time_ave/5000\n",
        "\n",
        "\n",
        "print('Average Penalty Per Epoch: ' +str(reward_tot))\n",
        "print('Average Steps Per Epoch: ' + str(steps_tot))\n",
        "print('Average Time Per Epoch: ' + str(time_ave))\n"
      ],
      "execution_count": 412,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Penalty Per Epoch: 62.9026\n",
            "Average Steps Per Epoch: 10.405\n",
            "Average Time Per Epoch: 0.0002188608004000912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFdtGY1tGggC"
      },
      "source": [
        "Indeed, with this more complex graph when we utilize Q-learning and increase the number of epochs we get much higher rewards (ie. lower penalty) and fewer steps than for the brute force approach. This also outperformed the brute force method even with fewer epochs (it outperformed even at like 500). Thus, it is clear that the reinforcement learning approach increases the effectiveness of the robber finding the cop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDZuSjRS944s"
      },
      "source": [
        "> **Bonus** Check that the learned policy satsifies the [Bellman Inequality](https://towardsdatascience.com/mathematical-analysis-of-reinforcement-learning-bellman-equation-ac9f0954e19f), i.e is the computed solution the actual optimal policy?"
      ]
    }
  ]
}