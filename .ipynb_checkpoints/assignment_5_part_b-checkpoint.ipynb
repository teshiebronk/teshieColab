{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "assignment_5_part_b-checkpoint.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SogY7a69yHFj"
      },
      "source": [
        "# To get access to the dataset:\n",
        "- If you already have the folder with the datasets, you might need to 'git pull' to ensure that it is updated\n",
        "> Else, clone repo using the command below <br>\n",
        "> \"git clone https://github.com/clemnyan/ENGS_108_Fall_2021.git\" <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWeOBUd1n2IC"
      },
      "source": [
        "**ENGS 108 Assignment 5 Checkpoint B**\n",
        "\n",
        "Teshie Bronk \n",
        "\n",
        "Professor Cybenko\n",
        "\n",
        "October 19, 2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8uBU28enwi6",
        "outputId": "745aab9f-44b9-4506-97ab-12d006000f4a"
      },
      "source": [
        "!git clone https://github.com/clemnyan/ENGS_108_Fall_2021.git "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ENGS_108_Fall_2021'...\n",
            "remote: Enumerating objects: 989, done.\u001b[K\n",
            "remote: Counting objects: 100% (989/989), done.\u001b[K\n",
            "remote: Compressing objects: 100% (948/948), done.\u001b[K\n",
            "remote: Total 989 (delta 42), reused 971 (delta 35), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (989/989), 77.54 MiB | 27.56 MiB/s, done.\n",
            "Resolving deltas: 100% (42/42), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITP8evQ-oEkP"
      },
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uBE3mbcyHFl"
      },
      "source": [
        "## **Problem 2: Introduction to TensorFlow**\n",
        "In this problem, we will start working in tensorflow to build deep learning systems starting with fully connected neural networks. We will focus on using the food image dataset we built in the last problem.\n",
        ">\n",
        "> **(a)** Using the food image dataset we built in the last problem (last week's assignment!), build a [tensorflow Data Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) that is shuffled with a batch size of 10. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APZQpgKuyHFl",
        "outputId": "6a0c3340-50e5-46bc-f7a1-8beb0c04a546"
      },
      "source": [
        "# Code and explanation\n",
        "path = '/content/ENGS_108_Fall_2021/datasets/ExampleFoodImageDataset'\n",
        "\n",
        "#create training set\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  path,\n",
        "  validation_split=0.2, #80% in training\n",
        "  subset=\"training\", #training set\n",
        "  seed=123, #set seed\n",
        "  image_size=(28, 28), #resize\n",
        "  batch_size=10 #set batch size)\n",
        "\n",
        "#create validation set\n",
        "valid_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  path,\n",
        "  validation_split=0.2, #20% in validation\n",
        "  subset=\"validation\", #validation set\n",
        "  seed=123, #set seed\n",
        "  image_size=(28, 28), #resize image\n",
        "  batch_size=10 #batch size)\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 978 files belonging to 9 classes.\n",
            "Using 783 files for training.\n",
            "Found 978 files belonging to 9 classes.\n",
            "Using 195 files for validation.\n",
            "['caesar_salad', 'caprese_salad', 'french_fries', 'greek_salad', 'hamburger', 'hot_dog', 'pizza', 'sashimi', 'sushi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEpOU-eryHFm"
      },
      "source": [
        "> **(b)** Build a two layer fully connected neural network of any size with a ReLu activation function and a final softmax layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbItCp4GwkhC",
        "outputId": "f78e1abd-6143-49b7-e293-2824921a35d9"
      },
      "source": [
        "#inspect image sets and ensure they're the correct size \n",
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break\n",
        "#our batches are of 10 with images of 28x28x3"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 28, 28, 3)\n",
            "(10,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTaFKAyXyP5b"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "#applying cache() and prefetch helps improve performance of neural net\n",
        "#additional documentation here: https://www.tensorflow.org/tutorials/load_data/images\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "valid_ds = valid_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a71bUHKyyHFm"
      },
      "source": [
        "#Creating a two layer fully connected neural network \n",
        "#to clarify, I interpreted this question as asking for ReLu, softmax, AND 2 additional layers\n",
        "\n",
        "#number of classes\n",
        "num_classes = 9\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Rescaling(1./255), #just rescaling from 0-->255 for rgb to 0-->1\n",
        "  tf.keras.layers.Dense(128, activation='relu'), #dense layer with 128 neurons, relu activation\n",
        "  tf.keras.layers.Flatten(), #flatten it out to 784 (ie. long vector of all pixels)\n",
        "  tf.keras.layers.Dense(10, activation='softmax') #softmax layer with 10 neurons. \n",
        "  #Creates probability distrib = 1 with probabilities for each of the classes]\n",
        ")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLqjII8JyHFm"
      },
      "source": [
        "> **(c)** Compile your model with an appropriate loss function and optimizer. Briefly describe your choices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQK9k7nR0eKW"
      },
      "source": [
        "#indicates optimizer, loss function, and metric for assessing model performance\n",
        "model.compile(\n",
        "  optimizer='adam', \n",
        "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy']) "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f-p13a9FdZ_"
      },
      "source": [
        "Updates model in response to particular loss function that we've chosen. The optimizer in this case, adam, uses a nonconstant learning rate. Adam also performs well in terms of memory usage, and general efficiency. Similarly, we are using a categorical cross entropy loss function here which computes the loss in accordance with the negative log of the probability of the correct classification. Ie. if probability of correct classification is 1 then loss is zero but if it's small (like 0.1) then the loss is higher - ie. further correcting the network. In turn, this lets for robust model fitting when there are number of classification options. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJQ3aNWxyHFn"
      },
      "source": [
        "> **(d)** Train your model on the food image training dataset. And report your accuracy on the testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U46nv6d0jCD",
        "outputId": "d716f52e-c30f-43ca-87fd-ffbe1d9cfd61"
      },
      "source": [
        "#fit the model of training data and assess on validation\n",
        "model.fit(\n",
        "  train_ds,\n",
        "  validation_data=valid_ds,\n",
        "  epochs=3\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4907: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 1s 13ms/step - loss: 2.2217 - accuracy: 0.3295 - val_loss: 1.5743 - val_accuracy: 0.4410\n",
            "Epoch 2/3\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 1.2862 - accuracy: 0.5211 - val_loss: 1.5369 - val_accuracy: 0.4154\n",
            "Epoch 3/3\n",
            "79/79 [==============================] - 1s 13ms/step - loss: 0.9949 - accuracy: 0.6501 - val_loss: 1.5494 - val_accuracy: 0.4205\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb8be80bed0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah39dzbQPZcj"
      },
      "source": [
        "After three epochs our neural net has a training accuracy of 0.65 and validation accuracy of 0.4205. Interestingly, our training accuracy increased across the epochs, but our test accuracy did not. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO0yejS6yHFo"
      },
      "source": [
        "> **(e)** Now try to tune this network by varying the number of layers, units, activations and see if you can outperform the network in part (d). Does your best model perform better or worse than the SVM in problem 1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3bGtadJyHFo",
        "outputId": "8706ee9c-fa34-4142-ad4e-26d4840a49ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Code and explanation\n",
        "\n",
        "model_2 = tf.keras.Sequential([\n",
        "  tf.keras.layers.Rescaling(1./255),\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation= 'softmax')\n",
        "  ])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7fb8d53f4cb0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 546, in __del__\n",
            "    handle=self._handle, deleter=self._deleter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1264, in delete_iterator\n",
            "    _ctx, \"DeleteIterator\", name, handle, deleter)\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlXl-UPAUMrh"
      },
      "source": [
        "#indicates optimizer, loss function, and metric for assessing model performance\n",
        "model_2.compile(\n",
        "  optimizer='adam', \n",
        "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy']) "
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pCj1FMcTmGH",
        "outputId": "39cca651-5bc1-4083-817c-0c8518b8e3cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#fit the model of training data and assess on validation\n",
        "model_2.fit(\n",
        "  train_ds,\n",
        "  validation_data = valid_ds,\n",
        "  epochs=3\n",
        ")"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4907: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 1s 13ms/step - loss: 2.4359 - accuracy: 0.3014 - val_loss: 1.8472 - val_accuracy: 0.2615\n",
            "Epoch 2/3\n",
            "79/79 [==============================] - 1s 11ms/step - loss: 1.8019 - accuracy: 0.3282 - val_loss: 1.7777 - val_accuracy: 0.3385\n",
            "Epoch 3/3\n",
            "79/79 [==============================] - 1s 11ms/step - loss: 1.7393 - accuracy: 0.3116 - val_loss: 1.7427 - val_accuracy: 0.3436\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb8b9f07150>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smYBaxrrR8vD"
      },
      "source": [
        "For this updated network, I added a number of layed to increase the network complexity. First, I tried adding 3 series of convolutional 2d layers followed by max pooling layers. The purpose of doing this was to extract certain features from the images with the convolutional layers then pass that through a max pooling layer to enhance those particular features. Furthermore, by having iterated layers of this we can do multiple steps of choosing those features and enhancing them to have a clear portrayal of what features are most important for classification. I also altered the filter size though this did not have a significant effect on classification accuracy. Finally, I included a flatten layer to flatten the pixels as I did in the previous model and implemented another softmax layer. \n",
        "\n",
        "In my first setup with 3 series of Conv2D+maxpooling layers, my validation accruacy (~30%) was low so I went back to just one set of Conv2D+maxpool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6fKFBUdyHFp"
      },
      "source": [
        "> **(BONUS)** We lost a lot of information when we resized the images in part (a). What would happen if we didn't resize the images and we built fit the neural network with all this other information? Try it out! *Hint: Runtime will be much longer, both to create the image dataset without resizing and to train the model, so you might have to get the code working and then just let it run.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VNEv-vMyHFp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhPzp9s2QEFt"
      },
      "source": [
        "If we didn't resize the images and built and fit the network with the entire images, we'd have much higher accuracy as there would be far more distinct features for our network to recognize in each image and pick up on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p827klpeyHFp"
      },
      "source": [
        "> **(BONUS)** Implement and explain other feature engineering (and data augmentation) techniques that we can perform to increase prediction accuracy? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knhllN7_yHFp"
      },
      "source": [
        "# Code and explanation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN6L76dkQ-30"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiqHNOC_yHFp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}