{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "assignment_5_part_b-checkpoint.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SogY7a69yHFj"
      },
      "source": [
        "# To get access to the dataset:\n",
        "- If you already have the folder with the datasets, you might need to 'git pull' to ensure that it is updated\n",
        "> Else, clone repo using the command below <br>\n",
        "> \"git clone https://github.com/clemnyan/ENGS_108_Fall_2021.git\" <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWeOBUd1n2IC"
      },
      "source": [
        "**ENGS 108 Assignment 5 Checkpoint B**\n",
        "\n",
        "Teshie Bronk \n",
        "\n",
        "Professor Cybenko\n",
        "\n",
        "October 19, 2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8uBU28enwi6",
        "outputId": "26eeab83-889c-435d-f826-e86e40373874",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/clemnyan/ENGS_108_Fall_2021.git "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ENGS_108_Fall_2021'...\n",
            "remote: Enumerating objects: 989, done.\u001b[K\n",
            "remote: Counting objects: 100% (989/989), done.\u001b[K\n",
            "remote: Compressing objects: 100% (948/948), done.\u001b[K\n",
            "remote: Total 989 (delta 42), reused 971 (delta 35), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (989/989), 77.54 MiB | 24.79 MiB/s, done.\n",
            "Resolving deltas: 100% (42/42), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITP8evQ-oEkP"
      },
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uBE3mbcyHFl"
      },
      "source": [
        "## **Problem 2: Introduction to TensorFlow**\n",
        "In this problem, we will start working in tensorflow to build deep learning systems starting with fully connected neural networks. We will focus on using the food image dataset we built in the last problem.\n",
        ">\n",
        "> **(a)** Using the food image dataset we built in the last problem (last week's assignment!), build a [tensorflow Data Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) that is shuffled with a batch size of 10. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APZQpgKuyHFl",
        "outputId": "701ae63a-ecd0-45e4-ef30-ae99187eeb55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Code and explanation\n",
        "path = '/content/ENGS_108_Fall_2021/datasets/ExampleFoodImageDataset'\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  path,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(28, 28),\n",
        "  batch_size=10)\n",
        "\n",
        "#train_ds = train_ds.apply(tf.data.experimental.ignore_errors())\n",
        "\n",
        "valid_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  path,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(28, 28),\n",
        "  batch_size=10)\n",
        "\n",
        "#valid_ds = valid_ds.apply(tf.data.experimental.ignore_errors())\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 978 files belonging to 9 classes.\n",
            "Using 783 files for training.\n",
            "Found 978 files belonging to 9 classes.\n",
            "Using 195 files for validation.\n",
            "['caesar_salad', 'caprese_salad', 'french_fries', 'greek_salad', 'hamburger', 'hot_dog', 'pizza', 'sashimi', 'sushi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEpOU-eryHFm"
      },
      "source": [
        "> **(b)** Build a two layer fully connected neural network of any size with a ReLu activation function and a final softmax layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbItCp4GwkhC",
        "outputId": "068550aa-eba5-43e6-f21e-3d5c44d9005f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break\n",
        "#see our batches are of 10 with images of 28x28x3\n",
        "\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255) #normalizing such that \n",
        "#rgb values are no long 0-->255 but are now 0 --> 1\n",
        "\n",
        "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "image_batch, labels_batch = next(iter(normalized_ds))\n",
        "first_image = image_batch[0]\n",
        "print(np.min(first_image), np.max(first_image)) #see now on 0-->1 rgb scale"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 28, 28, 3)\n",
            "(10,)\n",
            "0.0 0.99175674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTaFKAyXyP5b"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "valid_ds = valid_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a71bUHKyyHFm"
      },
      "source": [
        "# Code and explanation\n",
        "num_classes = 9\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Rescaling(1./255),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Softmax(),\n",
        "  tf.keras.layers.Dense(num_classes)\n",
        "])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLqjII8JyHFm"
      },
      "source": [
        "> **(c)** Compile your model with an appropriate loss function and optimizer. Briefly describe your choices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_03x7maOyHFn"
      },
      "source": [
        "# Code and explanation "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJQ3aNWxyHFn"
      },
      "source": [
        "> **(d)** Train your model on the food image training dataset. And report your accuracy on the testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJnaqyDOyHFo"
      },
      "source": [
        "# Code and explanation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO0yejS6yHFo"
      },
      "source": [
        "> **(e)** Now try to tune this network by varying the number of layers, units, activations and see if you can outperform the network in part (d). Does your best model perform better or worse than the SVM in problem 1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3bGtadJyHFo"
      },
      "source": [
        "# Code and explanation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6fKFBUdyHFp"
      },
      "source": [
        "> **(BONUS)** We lost a lot of information when we resized the images in part (a). What would happen if we didn't resize the images and we built fit the neural network with all this other information? Try it out! *Hint: Runtime will be much longer, both to create the image dataset without resizing and to train the model, so you might have to get the code working and then just let it run.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VNEv-vMyHFp"
      },
      "source": [
        "# Code and explanation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p827klpeyHFp"
      },
      "source": [
        "> **(BONUS)** Implement and explain other feature engineering (and data augmentation) techniques that we can perform to increase prediction accuracy? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knhllN7_yHFp"
      },
      "source": [
        "# Code and explanation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiqHNOC_yHFp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}